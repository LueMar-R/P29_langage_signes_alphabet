{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62cdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "model_rec = tf.keras.models.load_model('modelX2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1979acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '_'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LABELS = dict(zip([i for i in range(0, 26)], string.ascii_lowercase))\n",
    "LABELS[26] = \"_\"\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df6bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction d'identification du signe de la main grâce au modèle de classification\n",
    "def identify_crop(crop):\n",
    "    try :\n",
    "        resized = (cv2.resize(crop,(60,60)).astype('float') / 255).reshape(1,60,60,3)\n",
    "        recognized = np.argmax(model_rec.predict(resized)) \n",
    "        return recognized\n",
    "    except : pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ffccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction servant à trouver les limites du rectangle d'encadrement de la main\n",
    "def draw_rectangle(hand_landmarks, image):\n",
    "    image_height, image_width, _ = image.shape\n",
    "    keypoints = []\n",
    "    for data_point in hand_landmarks.landmark:\n",
    "        keypoints.append({\n",
    "                            'X': data_point.x*image_width,\n",
    "                            'Y': data_point.y*image_height,                            \n",
    "                            })\n",
    "    \n",
    "    X_min = image_width\n",
    "    X_max = 0\n",
    "    Y_min = image_height\n",
    "    Y_max = 0\n",
    "\n",
    "    for x in range(len(keypoints)):\n",
    "        if (keypoints[x]['X']) < X_min:\n",
    "            X_min = (keypoints[x]['X'])\n",
    "        if (keypoints[x]['X']) > X_max:\n",
    "            X_max = (keypoints[x]['X'])\n",
    "\n",
    "    for y in range(len(keypoints)):\n",
    "        if (keypoints[y]['Y']) < Y_min:\n",
    "            Y_min = (keypoints[y]['Y'])\n",
    "        if (keypoints[y]['Y']) > Y_max:\n",
    "            Y_max = (keypoints[y]['Y'])\n",
    "\n",
    "    startX = int(X_min -0.05* image_width)\n",
    "    startY = int(Y_min -0.05* image_height)\n",
    "    endX = int(X_max +0.05* image_width)\n",
    "    endY = int(Y_max +0.05* image_height)\n",
    "    \n",
    "    crop = image[startY:endY, startX:endX]\n",
    "    recognized = identify_crop(crop)\n",
    "    if recognized :\n",
    "        image = cv2.rectangle(image, (startX, startY), (endX, endY), (155, 255, 0), 2)\n",
    "        image = cv2.putText(image, LABELS[recognized], (10,300), cv2.FONT_HERSHEY_SIMPLEX, 2, (155, 255, 0), 2)\n",
    "    else :\n",
    "        image = cv2.rectangle(image, (startX, startY), (endX, endY), (122, 10, 122), 2)\n",
    "    return(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb97a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detection_video():    \n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(cv2.flip(image,1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image) # détection de la main grâce à mediapipe\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    image =  draw_rectangle(hand_landmarks, image)\n",
    "            cv2.imshow('Sign Language Detection', image)\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d043141",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c73b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
